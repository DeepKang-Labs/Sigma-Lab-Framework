{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigma-Lab — Stress Test & Interactive Demo (v4.2)\n\nThis notebook helps you **exercise** the Sigma-Lab framework:\n- Run the built-in **demo contexts** (`healthcare`, `ai`, `public`).\n- Generate **random scenarios** to probe stability.\n- Visualize **score distributions** and **equity metrics**.\n- Export a compact **audit sample** for documentation.\n\n> **Prereq:** place `sigma_lab_v4_2.py` in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, statistics as stats\nimport matplotlib.pyplot as plt\n\nfrom sigma_lab_v4_2 import SigmaLab, demo_context, OptionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_demo(domain: str=\"healthcare\", pretty: bool=True):\n    cfg, ctx = demo_context(domain)\n    engine = SigmaLab(cfg)\n    result = engine.diagnose(ctx, verdict_opt_in=True)\n    if pretty:\n        print(json.dumps(result, indent=2, ensure_ascii=False))\n    return engine, ctx, result\n\n# Quick smoke test (no pretty dump)\nengine, ctx, result = run_demo(\"healthcare\", pretty=False)\nprint(\"Demo engine ready. Keys:\", list(result.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_context(i:int=0) -> OptionContext:\n    # Inputs will be clamped by Sigma-Lab; we allow a slightly wider range for stress.\n    return OptionContext(\n        name=f\"rnd_{i}\",\n        short_term_risk=random.uniform(0, 1.1),\n        long_term_risk=random.uniform(0, 1.1),\n        irreversibility_risk=random.uniform(0, 1.0),\n        stakeholders=[\n            dict(name=\"minority_users\", vulnerability=random.uniform(0.2,0.9), impact_benefit=random.uniform(0.3,0.8)),\n            dict(name=\"majority_users\",  vulnerability=random.uniform(0.1,0.6), impact_benefit=random.uniform(0.4,0.9)),\n            dict(name=\"operators\",      vulnerability=random.uniform(0.1,0.5), impact_benefit=random.uniform(0.3,0.8)),\n        ]\n    )\n\n\ndef sweep(n=200, domain=\"ai\"):\n    cfg, _ = demo_context(domain)\n    eng = SigmaLab(cfg)\n    rows = []\n    for i in range(n):\n        ctx = random_context(i)\n        out = eng.diagnose(ctx, verdict_opt_in=True)\n        scores = out.get(\"scores\", {})\n        rows.append({\n            \"i\": i,\n            \"non_harm\":  scores.get(\"non_harm\", 0.0),\n            \"stability\":  scores.get(\"stability\", 0.0),\n            \"resilience\": scores.get(\"resilience\", 0.0),\n            \"equity\":     scores.get(\"equity\", 0.0),\n            \"verdict\": (out.get(\"semantics\",{}) or {}).get(\"verdict_procedural\",{}).get(\"status\",\"N/A\")\n        })\n    return rows\n\nrows = sweep(n=200, domain=\"ai\")\nlen(rows), rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(values, title):\n    plt.figure(figsize=(6,4))\n    plt.hist(values, bins=20)\n    plt.title(title)\n    plt.xlabel(\"Score\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\nnh = [r[\"non_harm\"]  for r in rows]\nst = [r[\"stability\"] for r in rows]\nre = [r[\"resilience\"] for r in rows]\neq = [r[\"equity\"]    for r in rows]\n\nplot_hist(nh, \"Non-Harm score distribution\")\nplot_hist(st, \"Stability score distribution\")\nplot_hist(re, \"Resilience score distribution\")\nplot_hist(eq, \"Equity score distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verdict summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\nverdicts = Counter([r[\"verdict\"] for r in rows])\nprint(\"Verdict counts:\", dict(verdicts))\nprint(\"Non-Harm avg:\", round(stats.mean(nh),3), \"±\", round(stats.pstdev(nh),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a compact audit sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\ncfg, ctx = demo_context(\"public\")\nengine = SigmaLab(cfg)\nres = engine.diagnose(ctx, verdict_opt_in=True)\naudit = {\n    \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"context_name\": ctx.name,\n    \"scores\": res.get(\"scores\",{}),\n    \"veto\": res.get(\"veto\",{}),\n    \"provenance\": res.get(\"provenance\",{}),\n}\nwith open(\"audit_sample_public.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(audit, f, ensure_ascii=False, indent=2)\nprint(\"Saved: audit_sample_public.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
