# -*- coding: utf-8 -*-
"""
Sigma-LLM v3.2 ‚Äî Web Interface (Gradio + Prometheus + Reflexive Mode)
Compatibilit√© : Workflow GitHub + Sigma-LLM v3.2
"""

import os, sys, importlib.util, json, time
import gradio as gr

# ================================
# üîπ Chargement robuste du module
# ================================
HERE = os.path.dirname(os.path.abspath(__file__))
CANDIDATE = os.path.join(HERE, "sigma_llm_complete.py")

try:
    from sigma_llm_complete import SigmaLLM
except ModuleNotFoundError:
    if os.path.exists(CANDIDATE):
        spec = importlib.util.spec_from_file_location("sigma_llm_complete", CANDIDATE)
        mod = importlib.util.module_from_spec(spec)
        sys.modules["sigma_llm_complete"] = mod
        spec.loader.exec_module(mod)
        SigmaLLM = mod.SigmaLLM
    else:
        raise FileNotFoundError("‚ùå Impossible de charger SigmaLLM : fichier sigma_llm_complete.py introuvable.")

# ================================
# üîπ Chargement du mod√®le
# ================================
MODEL_NAME = os.getenv("SIGMA_LLM_MODEL", "gpt2").strip() or "gpt2"

print(f"[App] D√©marrage Sigma-LLM sur mod√®le : {MODEL_NAME}")
try:
    agent = SigmaLLM(model_name=MODEL_NAME)
except Exception as e:
    print(f"[App] ‚ö†Ô∏è √âchec du chargement du mod√®le {MODEL_NAME} : {e}")
    print("[App] Tentative de fallback sur gpt2‚Ä¶")
    agent = SigmaLLM(model_name="gpt2")

# ================================
# üîπ Fonctions utilitaires
# ================================
def chat_fn(message, history):
    """Appel unique Sigma-LLM avec enregistrement conversationnel"""
    prompt = f"Human: {message}\nAI:"
    try:
        out = agent.generate(prompt, max_new_tokens=256)
        reply = out.split("AI:")[-1].strip()
        return reply
    except Exception as e:
        return f"[Erreur interne] {e}"

def export_metrics():
    """Affiche les derni√®res m√©triques Sigma"""
    report_path = os.path.join("reports", "sigma_llm_last_report.json")
    if os.path.exists(report_path):
        try:
            data = json.load(open(report_path, "r", encoding="utf-8"))
            summary = json.dumps(data, indent=2, ensure_ascii=False)
            return summary
        except Exception as e:
            return f"‚ö†Ô∏è Impossible de lire le rapport : {e}"
    else:
        return "Aucun rapport Sigma-LLM trouv√©."

def save_prompt(prompt):
    """Sauvegarde rapide du dernier prompt"""
    state_file = os.path.join("state", "last_prompt.txt")
    os.makedirs("state", exist_ok=True)
    with open(state_file, "w", encoding="utf-8") as f:
        f.write(prompt)
    return f"‚úÖ Prompt sauvegard√© ({len(prompt)} caract√®res)."

# ================================
# üîπ Interface Gradio enrichie
# ================================
with gr.Blocks(title="Sigma-LLM Reflexive Agent (v3.2)") as demo:
    gr.Markdown("## üß† Sigma-LLM Reflexive Agent ‚Äî v3.2")
    gr.Markdown(
        "- **S(t)** : Subjectivit√© dynamique  \n"
        "- **O(t)** : Objectivit√© pond√©r√©e  \n"
        "- **Œîcoh** : M√©ta-coh√©rence r√©flexive  \n"
        "- **Homeostasis** : contr√¥le entropique automatique  \n"
        "- **Prometheus** : export m√©triques (si activ√© via `SIGMA_PROM_PORT`)"
    )

    chat = gr.ChatInterface(fn=chat_fn, title="Dialogue r√©flexif Sigma-LLM")

    with gr.Accordion("üìä Outils de diagnostic Sigma-Lab", open=False):
        btn_metrics = gr.Button("Afficher derni√®res m√©triques")
        out_metrics = gr.Textbox(label="Dernier rapport Sigma", lines=15)
        btn_metrics.click(export_metrics, outputs=out_metrics)

        prompt_input = gr.Textbox(label="Dernier prompt √† sauvegarder", lines=2)
        btn_save = gr.Button("Sauvegarder ce prompt")
        out_save = gr.Textbox(label="√âtat sauvegarde")
        btn_save.click(save_prompt, inputs=prompt_input, outputs=out_save)

    gr.Markdown(
        f"**cwd :** `{os.getcwd()}`  \n"
        f"**sigma_llm_complete.py pr√©sent ?** `{os.path.exists(CANDIDATE)}`  \n"
        f"**Mod√®le actuel :** `{MODEL_NAME}`"
    )

# ================================
# üîπ Lancement du serveur
# ================================
if __name__ == "__main__":
    PORT = int(os.getenv("PORT", "7860"))
    HOST = os.getenv("HOST", "0.0.0.0")
    print(f"[App] Interface Gradio disponible sur http://{HOST}:{PORT}")
    demo.launch(server_name=HOST, server_port=PORT)
