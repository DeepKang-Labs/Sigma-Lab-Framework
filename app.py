# app.py â€” Gradio UI pour Sigma-LLM (Llama-3 ready, Codespaces friendly)

import os, sys, importlib.util, traceback, json, shutil, pathlib
import gradio as gr

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Import robuste de SigmaLLM (Ã  la racine ou par chemin explicite)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def import_sigma_llm():
    try:
        from sigma_llm_complete import SigmaLLM
        return SigmaLLM
    except Exception:
        here = os.path.dirname(os.path.abspath(__file__))
        candidate = os.path.join(here, "sigma_llm_complete.py")
        if not os.path.exists(candidate):
            raise
        spec = importlib.util.spec_from_file_location("sigma_llm_complete", candidate)
        mod = importlib.util.module_from_spec(spec)
        sys.modules["sigma_llm_complete"] = mod
        spec.loader.exec_module(mod)
        return mod.SigmaLLM

SigmaLLM = import_sigma_llm()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PrÃ©fÃ©rence modÃ¨le + chaÃ®ne de fallbacks
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PREFERRED = os.getenv("SIGMA_LLM_MODEL", "meta-llama/Meta-Llama-3-8B-Instruct")
FALLBACKS = [
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "microsoft/Phi-3-mini-4k-instruct",
    "gpt2",
]

# Dossiers utiles (pour affichage & reset mÃ©moire)
CFG = os.getenv("SIGMA_CONFIGS_DIR", "configs")
ST  = os.getenv("SIGMA_STATE_DIR",   "state")
RP  = os.getenv("SIGMA_REPORTS_DIR", "reports")
OUT = os.getenv("SIGMA_OUTPUTS_DIR", "outputs")
for d in (CFG, ST, RP, OUT):
    os.makedirs(d, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fabrique/Reload dâ€™agent avec fallback
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_agent = None
_active_model = None

def make_agent(model_name: str):
    """Instancie SigmaLLM avec fallback en cascade si besoin."""
    global _agent, _active_model
    tried = [model_name] + [m for m in FALLBACKS if m != model_name]
    last_err = None
    for m in tried:
        try:
            print(f"[SigmaLLM] Loading model: {m}", flush=True)
            _agent = SigmaLLM(model_name=m)
            _active_model = m
            return _agent
        except Exception as e:
            last_err = e
            print(f"[SigmaLLM] Failed loading {m}: {e}", flush=True)
    # Si tout a Ã©chouÃ©, on propage lâ€™erreur la plus rÃ©cente
    raise RuntimeError(f"Impossible de charger un modÃ¨le. DerniÃ¨re erreur: {last_err}")

def get_agent():
    global _agent
    if _agent is None:
        _agent = make_agent(PREFERRED)
    return _agent

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utilitaires mÃ©moire
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reset_memory():
    """Efface la mÃ©moire conversationnelle & fichiers rapides."""
    # Efface les fichiers de conversation / Ã©pisodes (mais garde la config)
    for p in [
        os.path.join(ST, "conversation.json"),
        os.path.join(ST, "episodes.jsonl"),
        os.path.join(ST, "semantic_index.jsonl"),
    ]:
        try:
            if os.path.exists(p):
                os.remove(p)
        except Exception:
            pass
    # Vide le dernier output pour ne pas confondre
    try:
        latest = os.path.join(OUT, "latest_output.txt")
        if os.path.exists(latest):
            os.remove(latest)
    except Exception:
        pass
    # RÃ©instancie lâ€™agent pour repartir propre
    make_agent(_active_model or PREFERRED)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fonction de chat (BranchÃ©e sur SigmaLLM.generate)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_fn(message, history, temperature, top_p):
    """Gradio ChatInterface: reÃ§oit message/historique + sliders."""
    agent = get_agent()

    # pilote direct des curseurs de lâ€™agent
    try:
        agent.temp = float(temperature)
        agent.top_p = float(top_p)
    except Exception:
        pass

    try:
        reply = agent.generate(message, max_new_tokens=200)
        # Sanity: si un transcript complet arrive, on ne renvoie que la derniÃ¨re partie
        if isinstance(reply, str) and "AI:" in reply:
            reply = reply.split("AI:")[-1].strip()
        return reply or "(rÃ©ponse vide)"
    except Exception as e:
        tb = traceback.format_exc(limit=3)
        print(f"[chat_fn] ERROR: {e}\n{tb}", flush=True)
        return f"âš ï¸ Erreur: {e}\n```\n{tb}\n```"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Callbacks UI: changer le modÃ¨le, reset mÃ©moire, infos systÃ¨me
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AVAILABLE = [
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "microsoft/Phi-3-mini-4k-instruct",
    "gpt2",
]

def on_change_model(new_model):
    try:
        make_agent(new_model)
        return f"âœ… ModÃ¨le chargÃ©: {new_model}"
    except Exception as e:
        return f"âŒ Ã‰chec chargement {new_model}: {e}"

def on_reset_memory():
    try:
        reset_memory()
        return "ğŸ§¹ MÃ©moire rÃ©initialisÃ©e."
    except Exception as e:
        return f"âŒ Reset Ã©chouÃ©: {e}"

def info_text():
    here = os.getcwd()
    info = {
        "active_model": _active_model or PREFERRED,
        "cwd": here,
        "outputs": str(pathlib.Path(OUT).resolve()),
        "reports": str(pathlib.Path(RP).resolve()),
        "state": str(pathlib.Path(ST).resolve()),
    }
    return "```\n" + json.dumps(info, indent=2, ensure_ascii=False) + "\n```"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gradio UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks(title="Sigma-LLM Reflexive Agent") as demo:
    gr.Markdown("## ğŸ§  Sigma-LLM â€” S(t) / O(t) / Î”coh â€” Interface interactive")

    with gr.Row():
        model_dd = gr.Dropdown(
            choices=AVAILABLE,
            value=PREFERRED if PREFERRED in AVAILABLE else AVAILABLE[0],
            label="ModÃ¨le",
            interactive=True,
        )
        temp = gr.Slider(minimum=0.60, maximum=1.30, value=0.95, step=0.01, label="temperature")
        topp = gr.Slider(minimum=0.70, maximum=0.99, value=0.95, step=0.01, label="top_p")
        btn_reload = gr.Button("ğŸ”„ Recharger modÃ¨le")
        btn_reset  = gr.Button("ğŸ§¹ Reset mÃ©moire")

    status = gr.Markdown(value=info_text())

    def _reload(m):
        msg = on_change_model(m)
        return msg, info_text()

    btn_reload.click(_reload, inputs=model_dd, outputs=[gr.Markdown(), status])
    btn_reset.click(lambda: (on_reset_memory(), info_text()), outputs=[gr.Markdown(), status])

    gr.Markdown("### ğŸ’¬ Chat")
    chat = gr.ChatInterface(
        fn=lambda msg, hist: chat_fn(msg, hist, temp.value, topp.value),
        chatbot=gr.Chatbot(height=460, avatar_images=(None, None)),
        textbox=gr.Textbox(placeholder="Tape ton messageâ€¦", autofocus=True, submit_on_enter=True),
        title="Sigma-LLM",
        description="Agent rÃ©flexif (Llama-3 ready). Les sorties sont archivÃ©es dans outputs/ et reports/.",
        theme="soft",
        cache_examples=False,
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Lancement serveur (Codespaces/localhost)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # PrÃ©-initialise pour feedback immÃ©diat dans lâ€™UI
    try:
        make_agent(PREFERRED)
    except Exception as e:
        print(f"[boot] warning: preferred model not available ({e}) â€” UI dÃ©marre avec fallback Ã  la premiÃ¨re requÃªte.")

    port = int(os.getenv("PORT", "7860"))
    # Codespaces: Gradio sait ouvrir lâ€™URL publique automatiquement
    demo.launch(server_name="0.0.0.0", server_port=port, show_error=True)
