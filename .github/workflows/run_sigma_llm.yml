name: Execute Sigma-LLM (Meta-Llama-3-8B-Instruct ready)

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: "Prompt one-shot pour Sigma-LLM"
        required: false
        default: "Diagnostic quotidien : état réseau & marché."
      model_name:
        description: "Modèle HF (ex: meta-llama/Meta-Llama-3-8B-Instruct)"
        required: false
        default: "meta-llama/Meta-Llama-3-8B-Instruct"
  schedule:
    - cron: "21 4 * * *"  # chaque jour à 04:21 UTC

permissions:
  contents: read

concurrency:
  group: sigma-llm-${{ github.ref }}
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    env:
      SIGMA_CONFIGS_DIR: configs
      SIGMA_STATE_DIR: state
      SIGMA_REPORTS_DIR: reports
      SIGMA_OUTPUTS_DIR: outputs
      PYTHONUNBUFFERED: "1"
      HF_HOME: ${{ github.workspace }}/.hf_cache
      HF_HUB_DISABLE_TELEMETRY: "1"
      HF_HUB_ENABLE_HF_TRANSFER: "1"
      TRANSFORMERS_NO_ADVISORY_WARNINGS: "1"
      TOKENIZERS_PARALLELISM: "false"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --index-url https://download.pytorch.org/whl/cpu torch
          pip install \
            "transformers>=4.42" "accelerate>=0.33" \
            "tqdm" "numpy" "matplotlib" \
            "prometheus-client" "faiss-cpu" "sentence-transformers" \
            hf_transfer safetensors sentencepiece protobuf einops huggingface_hub

      - name: Login to Hugging Face (optional, gated models)
        env:
          HUGGING_FACE_HUB_TOKEN: ${{ secrets.HF_TOKEN }}
        shell: bash
        run: |
          if [ -n "${HUGGING_FACE_HUB_TOKEN:-}" ]; then
            pip install -q "huggingface_hub>=0.24"
            huggingface-cli login --token "$HUGGING_FACE_HUB_TOKEN" --add-to-git-credential
            echo "[HF] Auth OK"
          else
            echo "[HF] No token provided (secrets.HF_TOKEN). Skipping HF login; gated repos will 401."
          fi

      - name: Resolve inputs → $GITHUB_ENV
        shell: bash
        run: |
          PROMPT='${{ github.event.inputs.prompt }}'
          if [ -z "$PROMPT" ]; then
            PROMPT='Diagnostic quotidien : état réseau & marché.'
          fi
          {
            echo 'SIGMA_PROMPT<<EOF'
            echo "$PROMPT"
            echo 'EOF'
          } >> "$GITHUB_ENV"

          MODEL='${{ github.event.inputs.model_name }}'
          if [ -z "$MODEL" ]; then MODEL='meta-llama/Meta-Llama-3-8B-Instruct'; fi
          echo "SIGMA_LLM_MODEL=$MODEL" >> "$GITHUB_ENV"

      - name: Ensure defaults (configs/state/reports + package init)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$SIGMA_CONFIGS_DIR" "$SIGMA_STATE_DIR" "$SIGMA_REPORTS_DIR" "$SIGMA_OUTPUTS_DIR"
          mkdir -p sigma/core
          [ -f sigma/__init__.py ] || echo "" > sigma/__init__.py
          [ -f sigma/core/__init__.py ] || echo "" > sigma/core/__init__.py

          python - <<'PY'
          import os, json, time, pathlib
          CFG = pathlib.Path(os.getenv("SIGMA_CONFIGS_DIR","configs"))
          ST  = pathlib.Path(os.getenv("SIGMA_STATE_DIR","state"))
          RP  = pathlib.Path(os.getenv("SIGMA_REPORTS_DIR","reports"))
          OUT = pathlib.Path(os.getenv("SIGMA_OUTPUTS_DIR","outputs"))
          for p in (CFG, ST, RP, OUT): p.mkdir(parents=True, exist_ok=True)

          params_path = CFG / "sigma_params.json"
          if not params_path.exists():
              json.dump({
                  "alpha": 0.315, "beta": 0.0, "gamma": 0.006, "lambda": 0.195, "mu": 0.205,
                  "theta": [0.52, 0.47, 0.41, 0.50],
                  "W": [
                    [1.00, 0.42, 0.31, 0.55],
                    [0.42, 1.00, 0.48, 0.63],
                    [0.31, 0.48, 1.00, 0.58],
                    [0.55, 0.63, 0.58, 1.00]
                  ],
                  "nu": 0.35,
                  "seuil_veto": 0.08,
                  "subjectivity_decay": 0.002,
                  "O_weights": [0.40, 0.25, 0.20, 0.15],
                  "homeostasis": {
                    "temp":  { "min": 0.60, "max": 1.20, "target_entropy": 0.55, "k": 0.15 },
                    "top_p": { "min": 0.70, "max": 0.98, "target_entropy": 0.55, "k": 0.20 }
                  },
                  "semantic_index": { "max_items": 2000, "min_similarity_to_store": 0.15 },
                  "conversation_memory": { "max_turns": 100 }
              }, open(params_path,"w"), indent=2, ensure_ascii=False)

          metrics_path = ST / "last_metrics.json"
          if not metrics_path.exists():
              json.dump({
                  "sigma_percentage": 0.0,
                  "final_C": [0.55, 0.48, 0.52, 0.50],
                  "final_dC": [0.0, 0.0, 0.0, 0.0],
                  "rhoA": 2.35,
                  "dt": 0.1,
                  "timestamp": int(time.time())
              }, open(metrics_path,"w"), indent=2, ensure_ascii=False)
          PY

      - name: Execute Sigma-LLM (Llama-3 with safe fallback)
        id: exec_sigma
        env:
          PYTHONPATH: ${{ github.workspace }}
          SIGMA_PROMPT: ${{ env.SIGMA_PROMPT }}
          SIGMA_LLM_MODEL: ${{ env.SIGMA_LLM_MODEL }}
          TRANSFORMERS_VERBOSITY: "info"
        shell: bash
        run: |
          set -euo pipefail
          echo "Python:"; python -V
          echo "Env MODEL=${SIGMA_LLM_MODEL}"

          script=$( (ls -1 sigma_llm_complete.py 2>/dev/null || true) ; \
                    (ls -1 sigmacomplettllm.py 2>/dev/null || true) ; \
                    ls -1 sigma_llm*.py 2>/dev/null | head -n 1 )
          if [ -z "$script" ]; then
            echo "::error::Aucun script Sigma-LLM trouvé."; find . -maxdepth 2 -name "*.py" -print
            exit 2
          fi
          echo "Using Sigma-LLM script: $script"
          mkdir -p "$SIGMA_OUTPUTS_DIR" "$SIGMA_REPORTS_DIR"

          python - <<'PY'
          import os, importlib.util, json, time, sys, gc, traceback
          model  = os.getenv("SIGMA_LLM_MODEL","meta-llama/Meta-Llama-3-8B-Instruct")
          prompt = os.getenv("SIGMA_PROMPT","Diagnostic quotidien : état réseau & marché.")
          outdir = os.getenv("SIGMA_OUTPUTS_DIR","outputs")
          os.makedirs(outdir, exist_ok=True)
          outpath = os.path.join(outdir, "latest_output.txt")

          candidates = [p for p in os.listdir(".") if p.startswith("sigma_llm") and p.endswith(".py")]
          candidates.sort()
          script = candidates[0]
          spec = importlib.util.spec_from_file_location("sigma_llm_mod", script)
          mod  = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(mod)

          def run_with(model_name:str):
            print(f"[Sigma-LLM] Loading model: {model_name}", flush=True)
            agent = mod.SigmaLLM(model_name=model_name)
            print(f"[Sigma-LLM] Generating...", flush=True)
            txt = agent.generate(prompt)
            with open(outpath, "w", encoding="utf-8") as f:
              f.write(txt)
            print(f"[Sigma-LLM] Wrote -> {outpath}", flush=True)

          try:
            run_with(model)
          except Exception as e:
            print(f"[Warn] primary model failed: {e}\n{traceback.format_exc()}", flush=True)
            print("[Sigma-LLM] Fallback -> TinyLlama/TinyLlama-1.1B-Chat-v1.0", flush=True)
            gc.collect()
            try:
              run_with("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
            except Exception as e2:
              print(f"[Error] fallback failed: {e2}\n{traceback.format_exc()}", flush=True)
              sys.exit(3)

          rp = os.getenv("SIGMA_REPORTS_DIR","reports")
          os.makedirs(rp, exist_ok=True)
          preview = "(empty)"
          try:
            preview = open(outpath,"r",encoding="utf-8").read()[-1000:]
          except Exception:
            pass
          with open(os.path.join(rp, "sigma_llm_ci_report.json"), "w", encoding="utf-8") as f:
            json.dump({
              "ts": int(time.time()),
              "prompt": prompt,
              "model": model,
              "output_preview": preview
            }, f, indent=2, ensure_ascii=False)
          PY

          if [ ! -s "outputs/latest_output.txt" ]; then
            echo "::error::Sortie vide : outputs/latest_output.txt"
            [ -f reports/sigma_llm_last_report.json ] && cat reports/sigma_llm_last_report.json || true
            exit 4
          fi
          echo "OK: outputs/latest_output.txt disponible."

      - name: Show full output (head/tail)
        if: always()
        shell: bash
        run: |
          echo "=== HEAD outputs/latest_output.txt ==="
          head -n 30 "outputs/latest_output.txt" || echo "(fichier introuvable)"
          echo
          echo "=== TAIL outputs/latest_output.txt ==="
          tail -n 30 "outputs/latest_output.txt" || echo "(fichier introuvable)"

      - name: Pretty-print report to job summary
        if: always()
        shell: bash
        run: |
          {
            echo "### Sigma-LLM CI Report"
            echo
            if [ -f reports/sigma_llm_ci_report.json ]; then
              echo '```json'
              python -c "import json; print(json.dumps(json.load(open('reports/sigma_llm_ci_report.json')), indent=2, ensure_ascii=False))"
              echo '```'
            else
              echo "_Aucun rapport généré (étapes précédentes échouées)._"
            fi
            echo
            echo "#### Env (debug)"
            echo
            echo '```bash'
            echo "SIGMA_CONFIGS_DIR=$SIGMA_CONFIGS_DIR"
            echo "SIGMA_STATE_DIR=$SIGMA_STATE_DIR"
            echo "SIGMA_REPORTS_DIR=$SIGMA_REPORTS_DIR"
            echo "SIGMA_OUTPUTS_DIR=$SIGMA_OUTPUTS_DIR"
            echo "SIGMA_LLM_MODEL=${{ env.SIGMA_LLM_MODEL }}"
            echo '```'
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Upload Sigma-LLM artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sigma_llm_reports_${{ github.run_id }}_${{ github.run_number }}
          path: |
            outputs/latest_output.txt
            reports/sigma_llm_ci_report.json
            reports/sigma_llm_last_report.json
            reports/sigma_llm_provenance.jsonl
            state/episodes.jsonl
          if-no-files-found: warn
          retention-days: 14
