name: Execute Sigma-LLM (CI-safe, one-shot autodiscover)

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: "Prompt one-shot pour Sigma-LLM"
        required: false
        default: "Diagnostic quotidien : état réseau & marché."
      model_name:
        description: "Modèle HF (ex: meta-llama/Meta-Llama-3-8B-Instruct)"
        required: false
        default: "meta-llama/Meta-Llama-3-8B-Instruct"
  schedule:
    - cron: "21 4 * * *"  # chaque jour à 04:21 UTC

permissions:
  contents: read

concurrency:
  group: sigma-llm-${{ github.ref }}
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      # Dossiers projet
      SIGMA_CONFIGS_DIR: configs
      SIGMA_STATE_DIR: state
      SIGMA_REPORTS_DIR: reports
      SIGMA_OUTPUTS_DIR: outputs

      # Sorties non-bufferisées
      PYTHONUNBUFFERED: "1"

      # Envs qualité/logs Transformers/HF
      HF_HUB_DISABLE_TELEMETRY: "1"
      HF_HUB_ENABLE_HF_TRANSFER: "1"   # nécessite le paquet hf_transfer
      TRANSFORMERS_NO_ADVISORY_WARNINGS: "1"
      TOKENIZERS_PARALLELISM: "false"

      # Cache local HF pour accélérer les runs
      HF_HOME: ${{ github.workspace }}/.hf_cache

      # (optionnel) Prometheus pour export métriques
      # SIGMA_PROM_PORT: "0"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --index-url https://download.pytorch.org/whl/cpu torch
          pip install \
            "transformers>=4.42" "accelerate>=0.33" \
            "tqdm" "numpy" "matplotlib" \
            "prometheus-client" "faiss-cpu" "sentence-transformers" \
            hf_transfer safetensors sentencepiece protobuf einops

      # ---- Resolve inputs → $GITHUB_ENV (évite la logique dans YAML) ----------
      - name: Resolve inputs → $GITHUB_ENV
        shell: bash
        run: |
          PROMPT='${{ github.event.inputs.prompt }}'
          if [ -z "$PROMPT" ]; then
            PROMPT='Diagnostic quotidien : état réseau & marché.'
          fi
          {
            echo 'SIGMA_PROMPT<<EOF'
            echo "$PROMPT"
            echo 'EOF'
          } >> "$GITHUB_ENV"

          MODEL='${{ github.event.inputs.model_name }}'
          if [ -z "$MODEL" ]; then MODEL='meta-llama/Meta-Llama-3-8B-Instruct'; fi
          echo "SIGMA_LLM_MODEL=$MODEL" >> "$GITHUB_ENV"

      - name: Ensure defaults (configs/state/reports + package init)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$SIGMA_CONFIGS_DIR" "$SIGMA_STATE_DIR" "$SIGMA_REPORTS_DIR" "$SIGMA_OUTPUTS_DIR"
          mkdir -p sigma/core
          [ -f sigma/__init__.py ] || echo "" > sigma/__init__.py
          [ -f sigma/core/__init__.py ] || echo "" > sigma/core/__init__.py

          python - <<'PY'
          import os, json, time, pathlib
          CFG = pathlib.Path(os.getenv("SIGMA_CONFIGS_DIR","configs"))
          ST  = pathlib.Path(os.getenv("SIGMA_STATE_DIR","state"))
          RP  = pathlib.Path(os.getenv("SIGMA_REPORTS_DIR","reports"))
          OUT = pathlib.Path(os.getenv("SIGMA_OUTPUTS_DIR","outputs"))
          for p in (CFG, ST, RP, OUT): p.mkdir(parents=True, exist_ok=True)

          params_path = CFG / "sigma_params.json"
          if not params_path.exists():
              json.dump({
                  "alpha": 0.315, "beta": 0.0, "gamma": 0.006, "lambda": 0.195, "mu": 0.205,
                  "theta": [0.52, 0.47, 0.41, 0.50],
                  "W": [
                    [1.00, 0.42, 0.31, 0.55],
                    [0.42, 1.00, 0.48, 0.63],
                    [0.31, 0.48, 1.00, 0.58],
                    [0.55, 0.63, 0.58, 1.00]
                  ],
                  "nu": 0.35,
                  "seuil_veto": 0.08,
                  "subjectivity_decay": 0.002,
                  "O_weights": [0.40, 0.25, 0.20, 0.15],
                  "homeostasis": {
                    "temp":  { "min": 0.60, "max": 1.20, "target_entropy": 0.55, "k": 0.15 },
                    "top_p": { "min": 0.70, "max": 0.98, "target_entropy": 0.55, "k": 0.20 }
                  },
                  "semantic_index": {
                    "max_items": 2000,
                    "min_similarity_to_store": 0.15
                  },
                  "conversation_memory": { "max_turns": 100 },
                  "meta": {
                    "source": "DeepSigma_Optimal_Regim_v1.0",
                    "timestamp": "2025-11-03T00:00:00Z",
                    "description": "Configuration v3.1: subjectivity_decay, O_weights, homeostasis, index sémantique, mémoire conv.",
                    "validated_by": "verify_invariants.yml"
                  }
              }, open(params_path,"w"), indent=2, ensure_ascii=False)

          metrics_path = ST / "last_metrics.json"
          if not metrics_path.exists():
              json.dump({
                  "sigma_percentage": 0.0,
                  "final_C": [0.55, 0.48, 0.52, 0.50],
                  "final_dC": [0.0, 0.0, 0.0, 0.0],
                  "rhoA": 2.35,
                  "dt": 0.1,
                  "timestamp": int(time.time())
              }, open(metrics_path,"w"), indent=2, ensure_ascii=False)
          PY

      # ---------- EXECUTION DIRECTE avec garde de debug ----------
      - name: Execute Sigma-LLM (python -u direct, debug guard)
        id: exec_sigma
        env:
          PYTHONPATH: ${{ github.workspace }}
          SIGMA_PROMPT: ${{ env.SIGMA_PROMPT }}
          SIGMA_LLM_MODEL: ${{ env.SIGMA_LLM_MODEL }}
          TRANSFORMERS_VERBOSITY: "info"
        shell: bash
        run: |
          set -euo pipefail
          echo "Python version:"; python -V

          echo "Pip check (key libs present?):"
          python - <<'PY'
          import pkgutil
          mods = ["torch","transformers","accelerate","hf_transfer","safetensors","sentencepiece","protobuf","einops"]
          print({m:bool(pkgutil.find_loader(m)) for m in mods})
          PY

          echo "Repo tree (top-level):"
          ls -la

          echo "Search candidate scripts:"
          candidates=( "sigma_llm_complete.py" "sigmacomplettllm.py" )
          for f in sigma_llm*.py SigmaLLM*.py; do
            [ -e "$f" ] && candidates+=("$f")
          done
          printf ' - %s\n' "${candidates[@]}"

          script=""
          for f in "${candidates[@]}"; do
            if [ -f "$f" ]; then script="$f"; break; fi
          done
          if [ -z "$script" ]; then
            echo "::error::Aucun script Sigma-LLM trouvé. Vérifie le nom exact du fichier."
            echo "Contenu détecté :"; find . -maxdepth 2 -type f -name "*.py" -print
            exit 2
          fi

          echo "Using Sigma-LLM script: $script"
          mkdir -p "$SIGMA_OUTPUTS_DIR" "$SIGMA_REPORTS_DIR"

          echo ">>> Running LLM with model='${SIGMA_LLM_MODEL}'"
          set -x
          python -u "$script" --prompt "$SIGMA_PROMPT" --model "$SIGMA_LLM_MODEL"
          set +x

          OUT="outputs/latest_output.txt"
          if [ ! -f "$OUT" ]; then
            echo "::warning::'$OUT' introuvable. On liste outputs/ :"
            ls -la outputs || true
          fi

          if [ -s "$OUT" ]; then
            echo "OK: sortie trouvée ($OUT)."
          else
            echo "::warning::Sortie vide ou absente. Diagnostic rapide:"
            [ -f reports/sigma_llm_last_report.json ] && \
              (echo "--- reports/sigma_llm_last_report.json ---"; cat reports/sigma_llm_last_report.json)
            [ -f reports/sigma_llm_provenance.jsonl ] && \
              (echo "--- tail reports/sigma_llm_provenance.jsonl ---"; tail -n 5 reports/sigma_llm_provenance.jsonl)
            echo "::error::Le LLM n'a pas produit de texte exploitable."
            exit 3
          fi

          # Petit rapport CI (aperçu) pour le Summary
          python - <<'PY'
          import os, json, time, pathlib
          RP  = os.getenv("SIGMA_REPORTS_DIR","reports")
          OUT = pathlib.Path(os.getenv("SIGMA_OUTPUTS_DIR","outputs")) / "latest_output.txt"
          OUT.parent.mkdir(parents=True, exist_ok=True)
          try:
              preview = OUT.read_text(encoding="utf-8")[-1000:]
          except Exception:
              preview = "(no latest_output.txt)"
          pathlib.Path(RP).mkdir(parents=True, exist_ok=True)
          with open(os.path.join(RP, "sigma_llm_ci_report.json"), "w", encoding="utf-8") as f:
              json.dump({
                "ts": int(time.time()),
                "prompt": os.getenv("SIGMA_PROMPT","(n/a)"),
                "model": os.getenv("SIGMA_LLM_MODEL","(n/a)"),
                "output_preview": preview
              }, f, indent=2, ensure_ascii=False)
          PY

      - name: Show full output (head/tail)
        if: always()
        shell: bash
        run: |
          echo "=== HEAD outputs/latest_output.txt ==="
          head -n 30 "outputs/latest_output.txt" || echo "(fichier introuvable)"
          echo
          echo "=== TAIL outputs/latest_output.txt ==="
          tail -n 30 "outputs/latest_output.txt" || echo "(fichier introuvable)"

      - name: Pretty-print report to job summary
        if: always()
        shell: bash
        run: |
          {
            echo "### Sigma-LLM CI Report"
            echo
            if [ -f reports/sigma_llm_ci_report.json ]; then
              echo '```json'
              python -c "import json; print(json.dumps(json.load(open('reports/sigma_llm_ci_report.json')), indent=2, ensure_ascii=False))"
              echo '```'
            else
              echo "_Aucun rapport généré (étapes précédentes échouées)._"
            fi
            echo
            echo "#### Env (debug)"
            echo
            echo '```bash'
            echo "SIGMA_CONFIGS_DIR=$SIGMA_CONFIGS_DIR"
            echo "SIGMA_STATE_DIR=$SIGMA_STATE_DIR"
            echo "SIGMA_REPORTS_DIR=$SIGMA_REPORTS_DIR"
            echo "SIGMA_OUTPUTS_DIR=$SIGMA_OUTPUTS_DIR"
            echo "SIGMA_LLM_MODEL=${{ env.SIGMA_LLM_MODEL }}"
            echo '```'
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Upload Sigma-LLM artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sigma_llm_reports_${{ github.run_id }}_${{ github.run_number }}
          path: |
            outputs/latest_output.txt
            reports/sigma_llm_ci_report.json
            reports/sigma_llm_last_report.json
            reports/sigma_llm_provenance.jsonl
            state/episodes.jsonl
          if-no-files-found: warn
          retention-days: 14
